{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://maxhalford.github.io/blog/bayesian-linear-regression/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two types of machine learning methods - online and traditional machine learning.\n",
    "Online model is dynamic and learnis on the fly.\n",
    "Building blocks of the bayesian machine learning are as follows - \n",
    "- Predictive Distribution $p(y_i|x_i)$ - distribution we want to obtain\n",
    "- Likelihood $p(y_i|x_i, \\theta_i)$ - given the model parameters, how realistic is it to get the pair $(x_i, y_i)$\n",
    "- Prior Distribution $p(\\theta_i)$\n",
    "\n",
    "Choosing a *good* prior is important. If not chosen properly we not get a tractable equation to update our model prameters and we'll need to depend upon techniques like **MCMC** and **variational inference**\n",
    "- Posterior Distribution $p(\\theta_{i+1}|\\theta{i}, x_i, y_i) \\propto p(y_i|x_i,\\theta_i)p(\\theta_i)$\n",
    "\n",
    "Now the predictive distribution can be computed as  - \n",
    "$$\n",
    "\\int p(y_i|\\boldsymbol{w}, x_i)p(\\boldsymbol{w})d\\boldsymbol{w}\n",
    "$$\n",
    "We have marginalized over the model parameters. This equation is intractible if we have not chosen the conjugate prior to the likelihood.\n",
    "\n",
    "Weâ€™re computing a weighted average of the potential $y_i$ values for each possible model parameter $\\boldsymbol{w}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online Belief Updating \n",
    "\n",
    "Whenever a new pair of $(x_i, y_i)$ arrives we update the distribution of parameters - \n",
    "$$\n",
    "p(\\theta_{i+1}|\\theta_{i}, x_i, y_i) \\propto p(x_i, y_i|\\theta_i)p(\\theta_i)\n",
    "$$\n",
    "\n",
    "Before any data comes in and we are asked to predict the $y_0$ - \n",
    "$$\n",
    "p(y_0|x_0) \\propto p(y_0|x_0, \\theta_0)p(\\theta_0)\n",
    "$$\n",
    "\n",
    "Ones we see the first output $y_0$ and hence the pair $(x_0, y_0)$, we can update our model parameters - \n",
    "$$\n",
    "p(\\theta_1|\\theta_0, x_0, y_0) \\propto p(x_0, y_0|\\theta_0)p(\\theta_0)\n",
    "$$\n",
    "\n",
    "Now again the predictive distribution for the output $y_1$ is given by -\n",
    "$$\n",
    "p(y_1|x_1) \\propto p(y_1|x_1, \\theta_1)p(\\theta_1|\\theta_0, x_0, y_0)\n",
    "$$\n",
    "\n",
    "i.e. the prior of the weights of the current iteration is the posterior of the weights of the previous iteration.\n",
    "\n",
    "Once we see the second output $y_1$, we can now update our prior again - \n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "p(\\theta_2|\\theta_1, x_1, y_1) & \\propto p(y_1|x_1, \\theta_1)p(\\theta_1) \\\\\n",
    "& \\propto p(y_1|x_1, \\theta_1)p(y_0|x_0, \\theta_0)p(\\theta_0)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$ \n",
    "\n",
    "similarly - \n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "p(\\theta_3|\\theta_2, x_1, y_1) &  \\propto p(y_2|x_2, \\theta_2)p(\\theta_2) \\\\\n",
    "& \\propto p(y_2|x_2, \\theta_2)p(y_1|x_1, \\theta_1)p(y_0|x_0, \\theta_0)p(\\theta_0)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "And so on and so forth. The posterior distribution at step $i+1$ becomes the prior at step $i$. **We only need to store the current distribution of weights to make everything work.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression (Bayesian)\n",
    "\n",
    "$$\n",
    "y_i = w_i^Tx_i + \\epsilon_i\n",
    "$$\n",
    "\n",
    "$y_i \\in \\boldsymbol{R}$,\n",
    "$x_i \\in \\boldsymbol{R}^d$,\n",
    "$w_i \\in \\boldsymbol{R}^d$\n",
    "\n",
    "$\\epsilon_i$ follows Gaussian Distribution. Hence the likelihood function is a gaussian distribution - \n",
    "$$\n",
    "y_i|(x_i, w_i) \\sim \\mathcal{N}(w_i^Tx_i, \\beta_i^{-1})\n",
    "$$\n",
    "$\\beta_i$ - precision >0 \n",
    "\n",
    "Prior distribution for the above likelihood function is the **multivariate gaussian distribtuion**\n",
    "$$\n",
    "w_0 \\sim \\mathcal{N}(m_0, S_0)\n",
    "$$\n",
    "$m_0$ is the mean of the distribution $\\in \\boldsymbol{R}^d$ <br>\n",
    "$S_0$ is the covariance matrix $\\in \\boldsymbol{R}^{d*d}$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\n",
    "m_0 & = (0, 0, ..., 0) \\\\\n",
    "S_0 & = diag(\\alpha^{-1}, \\alpha^{-1}, ..., \\alpha^{-1})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Now the posterior distribution of the weights is given by - \n",
    "$$\n",
    "\\begin{align*}\n",
    "p(w_{i+1}|w_i, x_i, y_i) & = \\mathcal{N}(m_{i+1}, S_{i+1}) \\\\\n",
    "S_{i+1} & = (S_{i+1} + \\beta x_ix_t^T)^{-1} \\\\\n",
    "m_{i+1} & = S_{i+1}(S_{i}^{-1}m_i + \\beta x_i^Ty_i)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "And the predictive distribution is given as follows - \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(y_i) & = \\mathcal{\\mu_i, \\sigma_i} \\\\\n",
    "\\mu_i & = w_i^Tx_i \\\\\n",
    "\\sigma_i & = \\frac{1}{\\beta} + x_i^TS_ix_i\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "class BayesLinReg:\n",
    "\n",
    "    def __init__(self, n_features, alpha, beta):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.n_features = n_features\n",
    "        self.mean = np.zeros(n_features)\n",
    "        self.cov = (1 / alpha) * np.eye(n_features)\n",
    "\n",
    "    def learn(self, x, y):\n",
    "        # Update the covariance matrix\n",
    "        cov_inv = self.cov_inv(x) + self.beta * np.outer(x, x)\n",
    "        # Update the mean vector\n",
    "        cov = np.linalg.inv(cov_inv)\n",
    "        mean = cov @ (self.cov_inv(x) @ self.mean + self.beta * y * x)\n",
    "\n",
    "        self.cov_inv = cov_inv\n",
    "        self.mean = mean\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, x):\n",
    "\n",
    "        # Obtain the predictive mean (Bishop eq. 3.58)\n",
    "        y_hat = x @ self.mean\n",
    "\n",
    "        # obtain the predictive variance (Bishop eq. 3.59)\n",
    "        w_cov = np.linalg.inv(self.cov_inv)\n",
    "        y_var = 1 / self.beta + x @ w_cov @ x.T\n",
    "\n",
    "        return stats.norm(y_hat, np.sqrt(y_var))\n",
    "\n",
    "    @property\n",
    "    def weights_dist(self):\n",
    "        cov = np.linalg.inv(self.cov_inv)\n",
    "        return stats.multivariate_normal(self.mean, cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
