{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smart_open import open\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import gensim\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_code = {\n",
    "    'EAP': 0,\n",
    "    'HPL': 1,\n",
    "    'MWS': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess(s):\n",
    "    x = unidecode.unidecode(s)\n",
    "    x = simple_preprocess(str(s), deacc=True)\n",
    "    x = [word for word in x if word not in stop_words]\n",
    "    #x = ''.join(e for e in x.lower() if (e.isalnum() or e.isspace()))\n",
    "    #x = ' '.join(lemmatizer.lemmatize(token) for token in x.split(\" \"))\n",
    "    #x = ' '.join(lemmatizer.lemmatize(token, \"v\") for token in x.split(\" \"))\n",
    "    #x = ' '.join(word for word in x.split(\" \") if not word in stop_words)\n",
    "    return x\n",
    "\n",
    "def get_tokenized_data(valid_ratio, X, y):\n",
    "    X = list(map(lambda x: preprocess(x), X))\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=valid_ratio, stratify=y)\n",
    "    return list(X_train), list(X_valid), list(y_train), list(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(df['text'])\n",
    "y = list(map(lambda x: label_code[x], df['author']))\n",
    "valid_ratio = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=valid_ratio, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_processed = list(map(lambda x: preprocess(x), X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17621"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17621"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams(words, bi_min=5, tri_min=2):\n",
    "    bigram = gensim.models.Phrases(words, min_count = bi_min)\n",
    "    bigram_model = gensim.models.phrases.Phraser(bigram)\n",
    "    return bigram_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = bigrams(X_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = [bigram[snipet] for snipet in X_processed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = gensim.corpora.Dictionary(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word.filter_extremes(no_below=5, no_above=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word.compactify()\n",
    "corpus = [id2word.doc2bow(text) for text in bigram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "logging.basicConfig(filename='lda_model.log', format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    lda_train4 = gensim.models.ldamulticore.LdaMulticore(\n",
    "                           corpus=corpus,\n",
    "                           num_topics=10,\n",
    "                           id2word=id2word,\n",
    "                           chunksize=100,\n",
    "                           workers=7, # Num. Processing Cores - 1\n",
    "                           passes=50,\n",
    "                           eval_every = 1,\n",
    "                           per_word_topics=True)\n",
    "    lda_train4.save('lda_train4.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3,\n",
       "  '0.012*\"one\" + 0.008*\"reason\" + 0.008*\"great\" + 0.007*\"little\" + 0.007*\"upon\" + 0.005*\"less\" + 0.005*\"idea\" + 0.005*\"make\" + 0.005*\"things\" + 0.005*\"believe\" + 0.005*\"old\" + 0.005*\"enough\"'),\n",
       " (7,\n",
       "  '0.012*\"like\" + 0.011*\"door\" + 0.009*\"came\" + 0.007*\"window\" + 0.007*\"sea\" + 0.007*\"open\" + 0.006*\"sky\" + 0.006*\"entered\" + 0.006*\"city\" + 0.006*\"still\" + 0.006*\"heavy\" + 0.005*\"wind\"'),\n",
       " (8,\n",
       "  '0.016*\"upon\" + 0.014*\"eyes\" + 0.008*\"like\" + 0.007*\"one\" + 0.007*\"saw\" + 0.006*\"head\" + 0.006*\"face\" + 0.006*\"lay\" + 0.006*\"could\" + 0.005*\"still\" + 0.005*\"seemed\" + 0.005*\"light\"'),\n",
       " (9,\n",
       "  '0.011*\"one\" + 0.008*\"street\" + 0.008*\"house\" + 0.007*\"time\" + 0.006*\"child\" + 0.006*\"three\" + 0.006*\"come\" + 0.006*\"though\" + 0.006*\"old\" + 0.006*\"town\" + 0.005*\"upon\" + 0.005*\"room\"'),\n",
       " (2,\n",
       "  '0.009*\"one\" + 0.008*\"death\" + 0.008*\"night\" + 0.007*\"could\" + 0.007*\"found\" + 0.007*\"last\" + 0.006*\"land\" + 0.005*\"would\" + 0.005*\"every\" + 0.005*\"time\" + 0.005*\"place\" + 0.004*\"two\"')]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_train4.print_topics(5,num_words=12)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs = []\n",
    "for i in range(len(X_processed)):\n",
    "    top_topics = lda_train4.get_document_topics(corpus[i], minimum_probability=0.0)\n",
    "    topic_vec = [top_topics[i][1] for i in range(5)]\n",
    "    topic_vec.extend([len(X_processed[i])]) # length review\n",
    "    train_vecs.append(topic_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8100033, 0.01000496, 0.010002675, 0.10996897, 0.010005308, 13]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vecs[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(train_vecs)\n",
    "_y = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import seaborn as sns\n",
    "%config InlineBackend.figure_formats = ['retina']\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import fbeta_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14096\n",
      "3525\n",
      "14097\n",
      "3524\n",
      "14097\n",
      "3524\n",
      "14097\n",
      "3524\n",
      "14097\n",
      "3524\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(5, shuffle=True, random_state=42)\n",
    "cv_lr_f1, cv_lrsgd_f1, cv_svcsgd_f1,  = [], [], []\n",
    "cv_lr_acc, cv_lrsgd_acc, cv_svcsgd_acc,  = [], [], []\n",
    "\n",
    "for train_ind, val_ind in kf.split(x, _y):\n",
    "    print(len(train_ind))\n",
    "    print(len(val_ind))\n",
    "    X_train, y_train = x[train_ind], _y[train_ind]\n",
    "    X_val, y_val = x[val_ind], _y[val_ind]\n",
    "    \n",
    "    # Scale Data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scale = scaler.fit_transform(X_train)\n",
    "    X_val_scale = scaler.transform(X_val)\n",
    "\n",
    "    # Logisitic Regression\n",
    "    lr = LogisticRegression(\n",
    "        class_weight= 'balanced',\n",
    "        solver='newton-cg',\n",
    "        fit_intercept=True\n",
    "    ).fit(X_train_scale, y_train)\n",
    "\n",
    "    y_pred = lr.predict(X_val_scale)\n",
    "    cv_lr_f1.append(f1_score(y_val, y_pred, average='weighted'))\n",
    "    cv_lr_acc.append(accuracy_score(y_val, y_pred))\n",
    "    \n",
    "    # Logistic Regression Mini-Batch SGD\n",
    "    sgd = linear_model.SGDClassifier(\n",
    "        max_iter=1000,\n",
    "        tol=1e-3,\n",
    "        loss='log',\n",
    "        class_weight='balanced'\n",
    "    ).fit(X_train_scale, y_train)\n",
    "    \n",
    "    y_pred = sgd.predict(X_val_scale)\n",
    "    cv_lrsgd_f1.append(f1_score(y_val, y_pred, average='weighted'))\n",
    "    cv_lrsgd_acc.append(accuracy_score(y_val, y_pred))\n",
    "    \n",
    "    # SGD Modified Huber\n",
    "    sgd_huber = linear_model.SGDClassifier(\n",
    "        max_iter=1000,\n",
    "        tol=1e-3,\n",
    "        alpha=20,\n",
    "        loss='modified_huber',\n",
    "        class_weight='balanced'\n",
    "    ).fit(X_train_scale, y_train)\n",
    "    \n",
    "    y_pred = sgd_huber.predict(X_val_scale)\n",
    "    cv_svcsgd_f1.append(f1_score(y_val, y_pred, average='weighted'))\n",
    "    cv_svcsgd_acc.append(accuracy_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4371631205673759,\n",
       " 0.43558456299659476,\n",
       " 0.4301929625425653,\n",
       " 0.44551645856980704,\n",
       " 0.438422247446084]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_lr_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
