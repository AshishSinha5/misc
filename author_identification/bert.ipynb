{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "\n",
    "# specify GPU\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import unidecode\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "label_code = {\n",
    "    'EAP': 0,\n",
    "    'HPL': 1,\n",
    "    'MWS': 2\n",
    "}\n",
    "def label_code_apply(x):\n",
    "    return label_code[x]\n",
    "df['author'] = df['author'].apply(label_code_apply)\n",
    "stop_words = set(stopwords.words(\"english\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punct(x):\n",
    "    return len([e for e in x.lower() if not (e.isalnum() or e.isspace())])\n",
    "\n",
    "def length(x):\n",
    "    return len(x)\n",
    "\n",
    "def preprocess(x):\n",
    "    x = unidecode.unidecode(x)\n",
    "    x = ''.join(e for e in x.lower() if (e.isalnum() or e.isspace()))\n",
    "    # x = ' '.join(lemmatizer.lemmatize(token) for token in x.split(\" \"))\n",
    "    # x = ' '.join(lemmatizer.lemmatize(token, \"v\") for token in x.split(\" \"))\n",
    "    x = ' '.join(word for word in x.split(\" \") if not word in stop_words)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_punct'] = df['text'].apply(punct)\n",
    "df['len'] = df['text'].apply(length)\n",
    "df['text_processed'] = df['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, temp_text, train_labels, temp_labels = train_test_split(df[['text_processed', 'len', 'num_punct']], df['author'],  \n",
    "                                                                    random_state=2018, \n",
    "                                                                    test_size=0.3, \n",
    "                                                                    stratify=df['author'])\n",
    "#train_labels = pd.Series(map(lambda x: label_code[x], train_labels))\n",
    "#temp_labels = pd.Series(map(lambda x: label_code[x], temp_labels))\n",
    "# we will use temp_text and temp_labels to create validation and test set\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
    "                                                                random_state=2018, \n",
    "                                                                test_size=0.5, \n",
    "                                                                stratify=temp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_std = StandardScaler()\n",
    "punct_std = StandardScaler()\n",
    "len_std.fit(train_text['len'].values.reshape(-1,1))\n",
    "punct_std.fit(train_text['len'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashis\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\ashis\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\ashis\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\ashis\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "train_text['len'] = len_std.transform(train_text['len'].values.reshape(-1,1))\n",
    "val_text['len'] = len_std.transform(val_text['len'].values.reshape(-1,1))\n",
    "test_text['len'] = len_std.transform(test_text['len'].values.reshape(-1,1))\n",
    "\n",
    "train_text['num_punct'] = punct_std.transform(train_text['num_punct'].values.reshape(-1,1))\n",
    "val_text['num_punct'] = punct_std.transform(val_text['num_punct'].values.reshape(-1,1))\n",
    "test_text['num_punct'] = punct_std.transform(test_text['num_punct'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import BERT-base pretrained model\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = bert.to(device)\n",
    "#op1 = bert(emb[:2], attention_mask = mask[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashis\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text['text_processed'].tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text['text_processed'].tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text['text_processed'].tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12486   -1.295198\n",
       "1421    -1.368867\n",
       "16353   -1.359658\n",
       "15073   -1.313615\n",
       "13658   -1.350450\n",
       "           ...   \n",
       "9531    -1.313615\n",
       "13329   -1.359658\n",
       "9367    -1.322824\n",
       "16296   -1.359658\n",
       "2993    -1.368867\n",
       "Name: num_punct, Length: 13705, dtype: float64"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text['num_punct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for train set\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_punct = torch.tensor(train_text['num_punct'].values, dtype=torch.float)\n",
    "train_len = torch.tensor(train_text['len'].values, dtype=torch.float)\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "# for validation set\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_punct = torch.tensor(val_text['num_punct'].values, dtype=torch.float)\n",
    "val_len = torch.tensor(val_text['len'].values, dtype=torch.float)\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "# for test set\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_punct = torch.tensor(test_text['num_punct'].values, dtype=torch.float)\n",
    "test_len = torch.tensor(test_text['len'].values, dtype=torch.float)\n",
    "test_y = torch.tensor(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 32\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_punct, train_len, train_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_punct, val_len, val_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all the parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "        super(BERT_Arch, self).__init__()\n",
    "        self.bert = bert \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        # relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    "        # dense layer 1\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "        # dense layer 2 (Output layer)\n",
    "        self.punc_emb = nn.Linear(1,12)\n",
    "        self.len_emb = nn.Linear(1,12)\n",
    "        self.fc2 = nn.Linear(512 + 12 + 12,3)\n",
    "        #softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask, punct, l):\n",
    "        #pass the inputs to the model  \n",
    "        bert_op = self.bert(sent_id, attention_mask=mask)\n",
    "        x = self.fc1(bert_op[1])\n",
    "        punct = self.punc_emb(punct.unsqueeze(1))\n",
    "        l = self.len_emb(l.unsqueeze(1))\n",
    "        x = torch.cat((x, punct, l), dim = -1)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        # output layer\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bert)\n",
    "\n",
    "# push the model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer from hugging face transformers\n",
    "from transformers import AdamW\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "cross_entropy  = nn.CrossEntropyLoss()\n",
    "\n",
    "# number of training epochs\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "      # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "      # iterate over batches\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "        # progress update after every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "        # push the batch to gpu\n",
    "        batch = [r.to(device) for r in batch]\n",
    "        sent_id, mask, punct, l, labels = batch\n",
    "        # clear previously calculated gradients \n",
    "        model.zero_grad()        \n",
    "        # get model predictions for the current batch\n",
    "        preds = model(sent_id, mask, punct, l)\n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = cross_entropy(preds, labels)\n",
    "        # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds=preds.detach().cpu().numpy()\n",
    "        # append the model predictions\n",
    "        total_preds.append(preds)\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    #returns the loss and predictions\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "\n",
    "    print(\"\\nEvaluating...\")\n",
    "    # deactivate dropout layers\n",
    "    model.eval()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "\n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(val_dataloader):\n",
    "\n",
    "    # Progress update every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            # elapsed = format_time(time.time() - t0)\n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "        # push the batch to gpu\n",
    "        batch = [t.to(device) for t in batch]\n",
    "        sent_id, mask, punct, l, labels = batch\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "            # model predictions\n",
    "            preds = model(sent_id, mask, punct, l)\n",
    "            # compute the validation loss between actual and predicted values\n",
    "            loss = cross_entropy(preds,labels)\n",
    "            total_loss = total_loss + loss.item()\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "            total_preds.append(preds)\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 1.080\n",
      "Validation Loss: 1.068\n",
      "\n",
      " Epoch 2 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 1.063\n",
      "Validation Loss: 1.053\n",
      "\n",
      " Epoch 3 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 1.049\n",
      "Validation Loss: 1.037\n",
      "\n",
      " Epoch 4 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 1.037\n",
      "Validation Loss: 1.024\n",
      "\n",
      " Epoch 5 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 1.024\n",
      "Validation Loss: 1.011\n",
      "\n",
      " Epoch 6 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 1.013\n",
      "Validation Loss: 0.999\n",
      "\n",
      " Epoch 7 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 1.004\n",
      "Validation Loss: 0.988\n",
      "\n",
      " Epoch 8 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 0.994\n",
      "Validation Loss: 0.978\n",
      "\n",
      " Epoch 9 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 0.986\n",
      "Validation Loss: 0.969\n",
      "\n",
      " Epoch 10 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 0.974\n",
      "Validation Loss: 0.961\n",
      "\n",
      " Epoch 11 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 0.969\n",
      "Validation Loss: 0.951\n",
      "\n",
      " Epoch 12 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 0.961\n",
      "Validation Loss: 0.943\n",
      "\n",
      " Epoch 13 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 0.953\n",
      "Validation Loss: 0.938\n",
      "\n",
      " Epoch 14 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 0.947\n",
      "Validation Loss: 0.932\n",
      "\n",
      " Epoch 15 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 0.942\n",
      "Validation Loss: 0.924\n",
      "\n",
      " Epoch 16 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 0.937\n",
      "Validation Loss: 0.917\n",
      "\n",
      " Epoch 17 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 0.930\n",
      "Validation Loss: 0.912\n",
      "\n",
      " Epoch 18 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 0.926\n",
      "Validation Loss: 0.906\n",
      "\n",
      " Epoch 19 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 0.923\n",
      "Validation Loss: 0.904\n",
      "\n",
      " Epoch 20 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 0.918\n",
      "Validation Loss: 0.898\n",
      "\n",
      " Epoch 21 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 0.916\n",
      "Validation Loss: 0.895\n",
      "\n",
      " Epoch 22 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 0.911\n",
      "Validation Loss: 0.890\n",
      "\n",
      " Epoch 23 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 0.906\n",
      "Validation Loss: 0.886\n",
      "\n",
      " Epoch 24 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 0.902\n",
      "Validation Loss: 0.885\n",
      "\n",
      " Epoch 25 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 0.901\n",
      "Validation Loss: 0.880\n",
      "\n",
      " Epoch 26 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 0.899\n",
      "Validation Loss: 0.877\n",
      "\n",
      " Epoch 27 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 0.897\n",
      "Validation Loss: 0.875\n",
      "\n",
      " Epoch 28 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 0.896\n",
      "Validation Loss: 0.874\n",
      "\n",
      " Epoch 29 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 0.890\n",
      "Validation Loss: 0.870\n",
      "\n",
      " Epoch 30 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 0.885\n",
      "Validation Loss: 0.866\n",
      "\n",
      " Epoch 31 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 0.883\n",
      "Validation Loss: 0.865\n",
      "\n",
      " Epoch 32 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 0.882\n",
      "Validation Loss: 0.862\n",
      "\n",
      " Epoch 33 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 0.885\n",
      "Validation Loss: 0.860\n",
      "\n",
      " Epoch 34 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 0.878\n",
      "Validation Loss: 0.858\n",
      "\n",
      " Epoch 35 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 0.876\n",
      "Validation Loss: 0.858\n",
      "\n",
      " Epoch 36 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 0.877\n",
      "Validation Loss: 0.854\n",
      "\n",
      " Epoch 37 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 0.877\n",
      "Validation Loss: 0.853\n",
      "\n",
      " Epoch 38 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 0.875\n",
      "Validation Loss: 0.850\n",
      "\n",
      " Epoch 39 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 0.868\n",
      "Validation Loss: 0.852\n",
      "\n",
      " Epoch 40 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 0.873\n",
      "Validation Loss: 0.850\n",
      "\n",
      " Epoch 41 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 0.869\n",
      "Validation Loss: 0.847\n",
      "\n",
      " Epoch 42 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 0.867\n",
      "Validation Loss: 0.844\n",
      "\n",
      " Epoch 43 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 0.869\n",
      "Validation Loss: 0.849\n",
      "\n",
      " Epoch 44 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 0.865\n",
      "Validation Loss: 0.842\n",
      "\n",
      " Epoch 45 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 0.864\n",
      "Validation Loss: 0.842\n",
      "\n",
      " Epoch 46 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n",
      "  Batch   150  of    429.\n",
      "  Batch   200  of    429.\n",
      "  Batch   250  of    429.\n",
      "  Batch   300  of    429.\n",
      "  Batch   350  of    429.\n",
      "  Batch   400  of    429.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "\n",
      "Training Loss: 0.864\n",
      "Validation Loss: 0.838\n",
      "\n",
      " Epoch 47 / 50\n",
      "  Batch    50  of    429.\n",
      "  Batch   100  of    429.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-325-c5138ddf6b25>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m#train model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m#evaluate model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-323-b51c471d14c1>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m# add on to the total loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[1;31m# backward pass to calculate the gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _ = train()\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, _ = evaluate()\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load weights of best model\n",
    "path = 'saved_weights.pt'\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for test data\n",
    "torch.cuda.empty_cache()\n",
    "model.to('cpu')\n",
    "#del train_data\n",
    "with torch.no_grad():\n",
    "    preds = model(test_seq, test_mask)\n",
    "    preds = preds.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.67      0.71      1185\n",
      "           1       0.75      0.62      0.68       846\n",
      "           2       0.62      0.81      0.70       906\n",
      "\n",
      "    accuracy                           0.70      2937\n",
      "   macro avg       0.71      0.70      0.70      2937\n",
      "weighted avg       0.71      0.70      0.70      2937\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>795</td>\n",
       "      <td>118</td>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>152</td>\n",
       "      <td>523</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>118</td>\n",
       "      <td>54</td>\n",
       "      <td>734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0    0    1    2\n",
       "row_0               \n",
       "0      795  118  272\n",
       "1      152  523  171\n",
       "2      118   54  734"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusion matrix\n",
    "pd.crosstab(test_y, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq = test_seq.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashis\\Anaconda3\\lib\\site-packages\\torch\\cuda\\memory.py:260: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.reset_max_memory_cached()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>id22965</td>\n",
       "      <td>A youth passed in solitude, my best years spen...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>id00912</td>\n",
       "      <td>I confess that neither the structure of langua...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>id16737</td>\n",
       "      <td>He shall find that I can feel my injuries; he ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>id12799</td>\n",
       "      <td>He had escaped me, and I must commence a destr...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19563</th>\n",
       "      <td>id10563</td>\n",
       "      <td>Yet from whom has not that rude hand rent away...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19566</th>\n",
       "      <td>id00832</td>\n",
       "      <td>These reflections made our legislators pause, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19569</th>\n",
       "      <td>id26790</td>\n",
       "      <td>Once my fancy was soothed with dreams of virtu...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19570</th>\n",
       "      <td>id14263</td>\n",
       "      <td>Nay, you may have met with another whom you ma...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19573</th>\n",
       "      <td>id07567</td>\n",
       "      <td>Stress of weather drove us up the Adriatic Gul...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6044 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                               text  author\n",
       "3      id27763  How lovely is spring As we looked from Windsor...       2\n",
       "5      id22965  A youth passed in solitude, my best years spen...       2\n",
       "9      id00912  I confess that neither the structure of langua...       2\n",
       "10     id16737  He shall find that I can feel my injuries; he ...       2\n",
       "15     id12799  He had escaped me, and I must commence a destr...       2\n",
       "...        ...                                                ...     ...\n",
       "19563  id10563  Yet from whom has not that rude hand rent away...       2\n",
       "19566  id00832  These reflections made our legislators pause, ...       2\n",
       "19569  id26790  Once my fancy was soothed with dreams of virtu...       2\n",
       "19570  id14263  Nay, you may have met with another whom you ma...       2\n",
       "19573  id07567  Stress of weather drove us up the Adriatic Gul...       2\n",
       "\n",
       "[6044 rows x 3 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df[df['author'] == 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
