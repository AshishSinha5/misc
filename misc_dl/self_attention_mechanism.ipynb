{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "X & = \\left[\\begin{array}{cccc}\n",
    "- - x_1 - - \\\\\n",
    "- - x_2 - - \\\\\n",
    ". . . . \\\\\n",
    ". . . . \\\\\n",
    "- - x_n - -\\\\\n",
    "\\end{array} \\\\\n",
    "\\right]_{n\\times d}\n",
    "& = \\left[\\begin{array}{ccc}\n",
    "x_{11}\\  x_{12}\\  ...\\   x_{1d} \\\\\n",
    "x_{21}\\  x_{22}\\  ...\\   x_{2d}  \\\\\n",
    ". . .  \\\\\n",
    ". . .  \\\\\n",
    "x_{n1}\\  x_{n2}\\  ...\\   x_{nd} \\\\\n",
    "\\end{array} \\\\ \n",
    "\\right]_{n\\times d}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Where - \n",
    "- $n$ - # tokens in the input sequence\n",
    "- $d$ - embedding dimension of the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: \n",
      " [[0.47403009 0.32876477 0.20495151 0.85971434]\n",
      " [0.80437388 0.22153859 0.88344645 0.47825417]\n",
      " [0.18688316 0.1481623  0.90946148 0.42144322]]\n",
      "Shape of input tokens:  (3, 4)\n"
     ]
    }
   ],
   "source": [
    "# Let there be n= 3 input tokens to the model with embedding dimension of d = 4\n",
    "# The input tokens are represented as a 3x4 matrix\n",
    "n = 3\n",
    "d = 4\n",
    "input_tokens = np.random.rand(n, d)\n",
    "print(\"Input tokens: \\n\", input_tokens)\n",
    "print(\"Shape of input tokens: \", input_tokens.shape)\n",
    "\n",
    "# each row of the input tokens is a vector of length 4 representing the embedding of the token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection Matrices for Query, Key and Value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align*}\n",
    "W_q = W_k = \\left[\\begin{array}{ccc}\n",
    "wq_{11}\\  wq_{12}\\  ...\\   wq_{1d_k} \\\\\n",
    "wq_{21}\\  wq_{22}\\  ...\\   wq_{2d_k}  \\\\\n",
    ". . .  \\\\\n",
    ". . .  \\\\\n",
    "wq_{d1}\\  wq_{d2}\\  ...\\   wq_{dd_k} \\\\\n",
    "\\end{array} \\\\ \n",
    "\\right]_{d \\times d_k}\n",
    "W_v = \\left[\\begin{array}{ccc}\n",
    "wv_{11}\\  wv_{12}\\  ...\\   wv_{1d_v} \\\\\n",
    "wv_{21}\\  wv_{22}\\  ...\\   wv_{2d_v}  \\\\\n",
    ". . .  \\\\\n",
    ". . .  \\\\\n",
    "wv_{d1}\\  wv_{d2}\\  ...\\   wv_{dd_v} \\\\\n",
    "\\end{array} \\\\ \n",
    "\\right]_{d \\times d_v}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Where - \n",
    "- $d$ - embedding dimension of the tokens\n",
    "- $d_k$ - embedding dimension of the query and key vectors\n",
    "- $d_v$ - embedding dimension of the value vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights for query and key projections vectors: \n",
      " [[0.85457913 0.72805367 0.52885905 0.81602111 0.11114425 0.16665275]\n",
      " [0.99075152 0.43915368 0.09446376 0.81835108 0.449025   0.76979672]\n",
      " [0.45029968 0.60978598 0.99083217 0.20000659 0.37349433 0.5733803 ]\n",
      " [0.68608398 0.72666931 0.09941451 0.34274698 0.34492009 0.53264535]]\n",
      "Shape of weights for query and key  projection vectors:  (4, 6)\n",
      "Weights for value  projection vector: \n",
      " [[0.36952055 0.22762371 0.03909977 0.43702857 0.80597927]\n",
      " [0.94430039 0.39320212 0.00445702 0.11603836 0.68048885]\n",
      " [0.30011805 0.76770143 0.00765135 0.02766898 0.96769012]\n",
      " [0.11024414 0.62303738 0.50907588 0.35974711 0.28597405]]\n",
      "Shape of weights for value projection vector:  (4, 5)\n"
     ]
    }
   ],
   "source": [
    "# Let's assume that the query and key vectors are of length 6\n",
    "# The value vector is of length 5\n",
    "dk = 6\n",
    "dv = 5\n",
    "# the weights for query and key vectors are represented as a 4x6 matrix\n",
    "weights_query_key = np.random.rand(d, dk)\n",
    "print(\"Weights for query and key projections vectors: \\n\", weights_query_key)\n",
    "print(\"Shape of weights for query and key  projection vectors: \", weights_query_key.shape)\n",
    "\n",
    "# the weights for the value vector is represented as a 4x5 matrix\n",
    "weights_value = np.random.rand(d, dv)\n",
    "print(\"Weights for value  projection vector: \\n\", weights_value)\n",
    "print(\"Shape of weights for value projection vector: \", weights_value.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Query, Key and Value matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We project the input tokens to query, key and value matrices using the projections defined in the previous step\n",
    "\n",
    "$$\\begin{align*}\n",
    "Q_{n \\times d_k} & = X_{n \\times d}W_{q\\ d \\times d_k} \\\\\n",
    "K_{n \\times d_k} & = X_{n \\times d}W_{k\\ d \\times d_k} \\\\\n",
    "V_{n \\times d_v} & = X_{n \\times d}W_{v\\ d \\times d_v} \\\\ \n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query vectors: \n",
      " [[1.41294625 1.23920219 0.57029209 0.99151971 0.57339029 0.90751846]\n",
      " [1.63282901 1.56916273 1.36922034 1.17829769 0.68379961 1.06588145]\n",
      " [1.00517413 1.06195371 1.05585209 0.60009606 0.57234251 0.89114652]]\n",
      "Shape of query vectors:  (3, 6)\n",
      "Key vectors: \n",
      " [[1.41294625 1.23920219 0.57029209 0.99151971 0.57339029 0.90751846]\n",
      " [1.63282901 1.56916273 1.36922034 1.17829769 0.68379961 1.06588145]\n",
      " [1.00517413 1.06195371 1.05585209 0.60009606 0.57234251 0.89114652]]\n",
      "Shape of key vectors:  (3, 6)\n",
      "Value vectors: \n",
      " [[0.64190468 0.93014723 0.45922777 0.56026456 1.04996474]\n",
      " [0.8242946  1.24639735 0.28266546 0.57373595 1.7907339 ]\n",
      " [0.52837434 1.06156653 0.22947264 0.27564264 1.25204546]]\n",
      "Shape of value vectors:  (3, 5)\n"
     ]
    }
   ],
   "source": [
    "# To compute the query, key and value vectors for each token, we take the dot product of the input tokens with the weights\n",
    "\n",
    "# query vectors\n",
    "query_vectors = np.dot(input_tokens, weights_query_key)\n",
    "print(\"Query vectors: \\n\", query_vectors)\n",
    "print(\"Shape of query vectors: \", query_vectors.shape)\n",
    "\n",
    "# key vectors\n",
    "key_vectors = np.dot(input_tokens, weights_query_key)\n",
    "print(\"Key vectors: \\n\", key_vectors)\n",
    "print(\"Shape of key vectors: \", key_vectors.shape)\n",
    "\n",
    "# value vectors\n",
    "value_vectors = np.dot(input_tokens, weights_value)\n",
    "print(\"Value vectors: \\n\", value_vectors)\n",
    "print(\"Shape of value vectors: \", value_vectors.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the Attention Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 - Calculate the similarity scores between the Query and Key vectors\n",
    "$$\n",
    "S_{n \\times n} = QK^T\n",
    "$$\n",
    "Where $s_{ij}$ = dot product similarity between query $q_i$ and key $k_j$ \n",
    "\n",
    "\n",
    "Step 2 - Normalize the similarity scores by the embedding dimension $d_k$\n",
    "$$\n",
    "S'_{n \\times n} = \\frac{S}{\\sqrt{d_k}}\n",
    "$$\n",
    "\n",
    "Step 3 - Scale the values so that the   \n",
    "$$\\begin{align*}\n",
    "a_{ij} & = \\frac{e^{s'_{ij}}}{\\Sigma_je^{s'_{ij}}} \\\\\n",
    "A_{n \\times n} & = softmax(\\frac{QK^T}{\\sqrt{d_k}})\n",
    "\\end{align*} \n",
    "$$\n",
    "So that - $\\Sigma_ja_{ij} = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity scores: \n",
      " [[5.06798984 3.09132164 3.47594607]\n",
      " [3.09132164 2.35205625 2.25159346]\n",
      " [3.47594607 2.25159346 2.57544933]]\n",
      "Shape of similarity scores:  (3, 3)\n",
      "Attention weights: \n",
      " [[0.50805787 0.22669918 0.26524295]\n",
      " [0.40828812 0.30192217 0.28978971]\n",
      " [0.43497103 0.26386552 0.30116346]]\n",
      "Shape of attention weights:  (3, 3)\n"
     ]
    }
   ],
   "source": [
    "# The dot product of the query and key vectors gives us the similarity between the query and key vectors\n",
    "similarity_scores = np.dot(query_vectors, key_vectors.T)\n",
    "print(\"Similarity scores: \\n\", similarity_scores)\n",
    "print(\"Shape of similarity scores: \", similarity_scores.shape)\n",
    "\n",
    "# The similarity scores are scaled by the square root of the dimension of the key vectors\n",
    "scaled_similarity_scores = similarity_scores / np.sqrt(6)\n",
    "\n",
    "# The scaled similarity scores are passed through a softmax function to get the attention weights\n",
    "attention_weights = np.exp(scaled_similarity_scores) / np.sum(np.exp(scaled_similarity_scores), axis=1, keepdims=True)\n",
    "print(\"Attention weights: \\n\", attention_weights)\n",
    "print(\"Shape of attention weights: \", attention_weights.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Contextualized Embeddings Using the attention weights\n",
    "\n",
    "$$\\begin{align*}\n",
    "X'_{n \\times d_v} & = A_{n \\times n}*V_{n \\times d_v}\n",
    "& = softmax(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "X'_{n \\times d_v} = \\left[\\begin{array}{cccc}\n",
    "- - x'_1 - - \\\\\n",
    "- - x'_2 - -  \\\\\n",
    ". . .  \\\\\n",
    ". . . \\\\\n",
    "- - x'_d - -  \\\\\n",
    "\\end{array} \\\\ \n",
    "\\right]_{n \\times d_v}  = \\left[\\begin{array}{ccc}\n",
    "a_{11}\\  a_{12}\\  ...\\   a_{1n} \\\\\n",
    "a_{21}\\  a_{22}\\  ...\\   a_{2n}  \\\\\n",
    ". . .  \\\\\n",
    ". . .  \\\\\n",
    "a_{n1}\\  a_{n2}\\  ...\\   a_{nn} \\\\\n",
    "\\end{array} \\\\ \n",
    "\\right]_{n \\times n} \\times \n",
    "\\left[\\begin{array}{ccc}\n",
    "v_{11}\\  v_{12}\\  ...\\   v_{1d_v} \\\\\n",
    "v_{21}\\  v_{22}\\  ...\\   v_{2d_v}  \\\\\n",
    ". . .  \\\\\n",
    ". . .  \\\\\n",
    "v_{n1}\\  v_{n2}\\  ...\\   a_{vd_v} \\\\\n",
    "\\end{array} \\\\ \n",
    "\\right]_{n \\times d_v} \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Where - \n",
    "$$\n",
    "\\Sigma_j{a_{ij}} = 1\n",
    "$$\n",
    "\n",
    "If for instance the value $a_{12}$ (i.e query 1 is most similar to key 2) is high the value of $x'_1$ is more influenced by the vector $v_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vectors: \n",
      " [[1.35183866 1.20284683 0.68933144 1.02996614 1.47134148]\n",
      " [1.30738598 1.16640691 0.69458305 1.00196277 1.37074831]\n",
      " [1.32670181 1.17789289 0.69657226 1.0192344  1.40538209]]\n",
      "Shape of context vectors:  (3, 5)\n"
     ]
    }
   ],
   "source": [
    "# The attention weights are multiplied with the value vectors to get the context vectors\n",
    "context_vectors = np.dot(attention_weights, value_vectors)\n",
    "print(\"Context vectors: \\n\", context_vectors)\n",
    "print(\"Shape of context vectors: \", context_vectors.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
